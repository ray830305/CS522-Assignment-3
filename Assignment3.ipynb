{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51500508",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fd7173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "import cv2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546f6ec8",
   "metadata": {},
   "source": [
    "# Data Collection, Preprocessing and Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3a3edb",
   "metadata": {},
   "source": [
    "## Reading in data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d52b2a",
   "metadata": {},
   "source": [
    "### Climbing down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b86a21",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path = \"Climbing_down_stairs/CL\"\n",
    "Climbing_down = []\n",
    "for i in range(1, 51):\n",
    "    df = pd.read_csv(path +f\"{i}\" + \"/Accelerometer.csv\" )\n",
    "    df = df.rename(columns={ df.columns[2]: \"ACCELEROMETER Z (m/s²)\" ,df.columns[3]: \"ACCELEROMETER Y (m/s²)\", \n",
    "                            df.columns[4]: \"ACCELEROMETER X (m/s²)\"   })\n",
    "    df = df[['seconds_elapsed', 'ACCELEROMETER X (m/s²)',\n",
    "   'ACCELEROMETER Y (m/s²)', 'ACCELEROMETER Z (m/s²)']]\n",
    "    df1 = pd.read_csv(path +f\"{i}\" + \"/Gyroscope.csv\" )\n",
    "    df1 = df1.rename(columns={ df1.columns[2]: \"GYROSCOPE Z (rad/s)\" ,df1.columns[3]: \"GYROSCOPE Y (rad/s)\", \n",
    "                            df1.columns[4]: \"GYROSCOPE X (rad/s)\"   })\n",
    "    df1 = df1[['time', 'seconds_elapsed', 'GYROSCOPE X (rad/s)',\n",
    "   'GYROSCOPE Y (rad/s)', 'GYROSCOPE Z (rad/s)']]\n",
    "    df1.drop(['time', 'seconds_elapsed'], axis=1, inplace=True)\n",
    "    Climbing = pd.concat([df, df1], axis = 1).values.tolist()\n",
    "    Climbing_down.append(Climbing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aed2894",
   "metadata": {},
   "source": [
    "### Standing up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c28fdf71",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path = \"standing_up/Stand\"\n",
    "Standing = []\n",
    "for i in range(1, 51):\n",
    "    df = pd.read_csv(path +f\"{i}\" + \"/Accelerometer.csv\" )\n",
    "    df = df.rename(columns={ df.columns[2]: \"ACCELEROMETER Z (m/s²)\" ,df.columns[3]: \"ACCELEROMETER Y (m/s²)\", \n",
    "                            df.columns[4]: \"ACCELEROMETER X (m/s²)\"   })\n",
    "    df = df[['seconds_elapsed', 'ACCELEROMETER X (m/s²)',\n",
    "   'ACCELEROMETER Y (m/s²)', 'ACCELEROMETER Z (m/s²)']]\n",
    "    df1 = pd.read_csv(path +f\"{i}\" + \"/Gyroscope.csv\" )\n",
    "    df1 = df1.rename(columns={ df1.columns[2]: \"GYROSCOPE Z (rad/s)\" ,df1.columns[3]: \"GYROSCOPE Y (rad/s)\", \n",
    "                            df1.columns[4]: \"GYROSCOPE X (rad/s)\"   })\n",
    "    df1 = df1[['time', 'seconds_elapsed', 'GYROSCOPE X (rad/s)',\n",
    "   'GYROSCOPE Y (rad/s)', 'GYROSCOPE Z (rad/s)']]\n",
    "\n",
    "    df1.drop(['time', 'seconds_elapsed'], axis=1, inplace=True)\n",
    "\n",
    "    Stand = pd.concat([df, df1], axis = 1 , join='inner').values.tolist()\n",
    "    Standing.append(Stand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28275d6",
   "metadata": {},
   "source": [
    "### Walking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41002e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/walking/walking_\"\n",
    "walking = []\n",
    "for i in range(20):\n",
    "    df = pd.read_csv(path +f\"{i}.csv\",header=1,sep=\";\")\n",
    "    df = df[['Time since start in ms ', 'ACCELEROMETER X (m/s²)',\n",
    "      'ACCELEROMETER Y (m/s²)', 'ACCELEROMETER Z (m/s²)', 'GYROSCOPE X (rad/s)', \n",
    "    'GYROSCOPE Y (rad/s)', 'GYROSCOPE Z (rad/s)']]\n",
    "    df['Time since start in ms ']/= 1000\n",
    "    df = df.rename(columns={ df.columns[0]: \"seconds_elapsed\"}).values.tolist()\n",
    "    walking.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c13c6f0",
   "metadata": {},
   "source": [
    "### Rowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23f59de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rowing = pd.read_csv(\"RowingMachine_20221102_210235_AndroSensor.csv\",header=1,sep=\";\")\n",
    "rowing = rowing[['Time since start in ms ', 'ACCELEROMETER X (m/s²)',\n",
    "      'ACCELEROMETER Y (m/s²)', 'ACCELEROMETER Z (m/s²)', 'GYROSCOPE X (rad/s)', \n",
    "    'GYROSCOPE Y (rad/s)', 'GYROSCOPE Z (rad/s)']]\n",
    "rowing['Time since start in ms ']/= 1000\n",
    "rowing = [rowing.rename(columns={ rowing.columns[0]: \"seconds_elapsed\"}).values.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb305491",
   "metadata": {},
   "source": [
    "### Climbing up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0424fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "climbing_up = pd.read_csv(\"climbing_Up_stairs.csv\",header=1,sep=\";\")\n",
    "climbing_up = climbing_up[['Time since start in ms ', 'ACCELEROMETER X (m/s²)',\n",
    "      'ACCELEROMETER Y (m/s²)', 'ACCELEROMETER Z (m/s²)', 'GYROSCOPE X (rad/s)', \n",
    "    'GYROSCOPE Y (rad/s)', 'GYROSCOPE Z (rad/s)']]\n",
    "climbing_up['Time since start in ms ']/= 1000\n",
    "climbing_up = [climbing_up.rename(columns={ climbing_up.columns[0]: \"seconds_elapsed\"}).values.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc62504b",
   "metadata": {},
   "source": [
    "### Create data list\n",
    "The data is sorted by it class, with the order: walking, rowing, climbing down, climbing up and standing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b33909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All data [walking,rowing,climbing_down,climbing_up,standing]\n",
    "data = [walking, rowing, Climbing_down, climbing_up, Standing]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983434e2",
   "metadata": {},
   "source": [
    "## Removing start and end period\n",
    "Some activities are more prown to wrong values at the beginning and end, because you have to move the phone to start/stop the recording. Therefore are the first five and last five seconds ignored for these activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3ab0668",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0, 1, 3]:\n",
    "    for j in range(len(data[i])):\n",
    "        data[i][j] = data[i][j][10:-10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1109f7a",
   "metadata": {},
   "source": [
    "## Windowing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f2ace1",
   "metadata": {},
   "source": [
    "For activities that are not instance based, the data has to be windowed.\n",
    "For walking a window size of 2 minutes is chosen and for rowing and climbing up a window size of 1 minute, because they have less data overall. A overlap of 80% is chosen (seemed to work best after testing different overlaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7828b16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = {0: 240, 1: 120, 3: 120}  # In indexes, one index is equal to 0.5 seconds\n",
    "overlap = 0.8\n",
    "\n",
    "for i in [0, 1, 3]:\n",
    "    windows = []\n",
    "    for j in range(len(data[i])):\n",
    "        end_index = window_length[i]\n",
    "        window_number = 1\n",
    "        while end_index - 1 < len(data[i][j]):\n",
    "            window = data[i][j][end_index - window_length[i]:end_index]\n",
    "            windows.append(window)\n",
    "            window_number += 1\n",
    "            end_index += int((1 - overlap) * window_length[i])\n",
    "    data[i] = windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84406c4b",
   "metadata": {},
   "source": [
    "## Spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a471d4b8",
   "metadata": {},
   "source": [
    "To extract the frequency information over time from the data, spectograms are created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c08e68e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fs_values = {0: 2, 1: 2, 2: 100, 3: 2, 4: 200}\n",
    "FFT_SIZE=1024\n",
    "data_spectrograms = [[] for _ in range(len(data))]\n",
    "for cls_number in range(len(data)):\n",
    "    for i in range(len(data[cls_number])):\n",
    "        features_spectograms = []\n",
    "        for j in range(1, 7):\n",
    "            data_points = np.asarray([x[j] for x in data[cls_number][i]])\n",
    "            nperseg = FFT_SIZE if len(data_points) > FFT_SIZE else len(data_points)\n",
    "            f,t,pxx = signal.spectrogram(data_points, nperseg=nperseg, fs=fs_values[cls_number] , noverlap=nperseg/2)\n",
    "            features_spectograms.append([f, t, pxx])\n",
    "        data_spectrograms[cls_number].append(features_spectograms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bed635e",
   "metadata": {},
   "source": [
    "## Binning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e262f2e",
   "metadata": {},
   "source": [
    "To keep as much information of the spectrograms as possible, but without having to much features, a semi high bin number is chosen. Testing and comparing different numbers of bins and its results showed that 50 in each axis seems to be a good mix between these two requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1c3d34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_freq_bins=50\n",
    "num_time_bins=50\n",
    "data_spectrograms_binned = [[] for _ in range(len(data))]\n",
    "for cls_number in range(len(data_spectrograms)):\n",
    "    for i in range(len(data_spectrograms[cls_number])):\n",
    "        features_spectograms = []\n",
    "        for j in range(6):\n",
    "            resized_pxx = cv2.resize(data_spectrograms[cls_number][i][j][2],(num_time_bins,num_freq_bins))\n",
    "            features_spectograms.append([data_spectrograms[cls_number][i][j][0], data_spectrograms[cls_number][i][j][1], resized_pxx])\n",
    "        data_spectrograms_binned[cls_number].append(features_spectograms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8972e7f",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8337c606",
   "metadata": {},
   "source": [
    "As features only the binned spectrograms for all values (acceleration x, acceleration y, acceleration z, gyroscope x, gyroscope y and gyroscope z) are chosen. This is enough to achieve a sufficient accuracy with a Random Forest and SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e561ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features = [[] for _ in range(len(data))]\n",
    "\n",
    "for cls_number in range(len(data_spectrograms_binned)):\n",
    "    for i in range(len(data_spectrograms_binned[cls_number])):\n",
    "        features = []\n",
    "        for j in range(6):\n",
    "            features.extend(data_spectrograms_binned[cls_number][i][j][2].reshape((-1,)).tolist())\n",
    "        data_features[cls_number].append(np.asarray(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c79cc8",
   "metadata": {},
   "source": [
    "## Creating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "befb6e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "labels_list = []\n",
    "\n",
    "for cls_number in range(len(data_features)):\n",
    "    for i in range(len(data_features[cls_number])):\n",
    "        data_list.append(data_features[cls_number][i])\n",
    "        labels_list.append(cls_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da36376",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3aded3",
   "metadata": {},
   "source": [
    "Sligthly improves testing accuracy of SVM by 0.5 % and has no effect on testing accuracy of the Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb0581d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data_list = scaler.fit_transform(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e63443a",
   "metadata": {},
   "source": [
    "## Create training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bd8be5",
   "metadata": {},
   "source": [
    "A 70/30 split is chosen, which means that 70% of the data is used for training and 30% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebb16be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(data_list, labels_list, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63a2398",
   "metadata": {},
   "source": [
    "# ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2f1e51",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b97cd076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cross Validation Score from Training:\n",
      "0.9980392156862745\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[135   0   0   0   0]\n",
      " [  0  33   0   0   0]\n",
      " [  0   0  13   0   0]\n",
      " [  0   0   0  24   0]\n",
      " [  0   0   0   0  13]]\n",
      "\n",
      "\n",
      "Test Statistics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       135\n",
      "           1       1.00      1.00      1.00        33\n",
      "           2       1.00      1.00      1.00        13\n",
      "           3       1.00      1.00      1.00        24\n",
      "           4       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00       218\n",
      "   macro avg       1.00      1.00      1.00       218\n",
      "weighted avg       1.00      1.00      1.00       218\n",
      "\n",
      "\n",
      "\n",
      "Testing Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier()\n",
    "clf.fit(xtrain, ytrain)\n",
    "cv_scores = cross_val_score(clf, xtrain, ytrain, cv=10)\n",
    "print('Average Cross Validation Score from Training:', cv_scores.mean(), sep='\\n', end='\\n\\n\\n')\n",
    "\n",
    "ypred = clf.predict(xtest)\n",
    "cm = confusion_matrix(ytest, ypred)\n",
    "cr = classification_report(ytest, ypred)\n",
    "\n",
    "print('Confusion Matrix:', cm, sep='\\n', end='\\n\\n\\n')\n",
    "print('Test Statistics:', cr, sep='\\n', end='\\n\\n\\n')\n",
    "\n",
    "print('Testing Accuracy:', accuracy_score(ytest, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f390525b",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be25132f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cross Validation Score from Training:\n",
      "0.9900784313725491\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[135   0   0   0   0]\n",
      " [  0  33   0   0   0]\n",
      " [  0   0  13   0   0]\n",
      " [  0   0   0  24   0]\n",
      " [  0   0   0   0  13]]\n",
      "\n",
      "\n",
      "Test Statistics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       135\n",
      "           1       1.00      1.00      1.00        33\n",
      "           2       1.00      1.00      1.00        13\n",
      "           3       1.00      1.00      1.00        24\n",
      "           4       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00       218\n",
      "   macro avg       1.00      1.00      1.00       218\n",
      "weighted avg       1.00      1.00      1.00       218\n",
      "\n",
      "\n",
      "\n",
      "Testing Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel=\"linear\")\n",
    "clf.fit(xtrain, ytrain)\n",
    "cv_scores = cross_val_score(clf, xtrain, ytrain, cv=10)\n",
    "print('Average Cross Validation Score from Training:', cv_scores.mean(), sep='\\n', end='\\n\\n\\n')\n",
    "\n",
    "ypred = clf.predict(xtest)\n",
    "cm = confusion_matrix(ytest, ypred)\n",
    "cr = classification_report(ytest, ypred)\n",
    "\n",
    "print('Confusion Matrix:', cm, sep='\\n', end='\\n\\n\\n')\n",
    "print('Test Statistics:', cr, sep='\\n', end='\\n\\n\\n')\n",
    "\n",
    "print('Testing Accuracy:', accuracy_score(ytest, ypred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
